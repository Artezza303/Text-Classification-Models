{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Embedding, Flatten\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset from https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "#too big to upload from browser into github\n",
    "df = pd.read_csv('IMDB Dataset.csv',encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['review'].values\n",
    "y = df['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through every sentence and filter our capitalizations,periods,double spaces, etc. \n",
    "for i, s in enumerate(sentences):\n",
    "    soup = BeautifulSoup(sentences[i], \"html.parser\")\n",
    "    sentences[i] = soup.get_text()\n",
    "    sentences[i]= re.sub('\\[[^]]*\\]', ' ', sentences[i])\n",
    "    sentences[i] = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    sentences[i] = re.sub(' +', ' ', sentences[i])\n",
    "    sentences[i] = sentences[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 4, 1, 77, 46, 1051, 11, 100, 149, 41, 394, 20, 230, 29, 32, 25, 204, 14, 10, 6, 612, 47, 590, 17, 68, 1, 88, 148, 11, 68, 44, 13, 91, 2, 135, 4, 559, 61, 265, 8, 204, 37, 1, 647, 141, 1721, 68, 10, 6, 23, 3, 116, 16, 1, 40, 10, 116, 56, 17, 5, 1455, 371, 40, 559, 91, 6, 8, 1, 355, 356, 4, 1, 647, 7, 6, 432, 14, 11, 6, 1, 357, 5, 1, 1030, 7, 1399, 22, 518, 34, 4, 1, 1183, 115, 30, 1, 27, 2, 385, 36, 6, 23, 297, 22, 1, 518, 6, 341, 5, 107, 2, 52, 36, 324, 2, 25, 111, 223, 240, 9, 60, 132, 1, 280, 1315, 4, 1, 116, 6, 677, 5, 1, 192, 11, 7, 266, 115, 77, 273, 570, 21, 819, 182, 1292, 16, 1214, 819, 1420, 819, 865, 152, 21, 939, 184, 1, 88, 394, 9, 123, 210, 68, 14, 36, 1606, 7, 13, 9, 411, 21, 132, 9, 13, 1568, 16, 7, 18, 14, 9, 290, 52, 9, 1403, 3, 1255, 16, 2, 190, 5, 1, 297, 4, 559, 23, 41, 559, 18, 35, 230, 29, 43, 16, 3, 35, 230, 494, 22, 627, 2, 75, 240, 17, 7, 69, 638, 694, 109, 649, 83, 1183, 677, 5, 66, 564, 4, 891, 1999, 40, 1183, 549, 149, 20, 197, 425, 17, 47, 6, 795, 1582, 45, 20, 50, 75, 8, 1201, 17, 126, 479]\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(sentences)\n",
    "print(X[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape =  (50000, 1934)\n"
     ]
    }
   ],
   "source": [
    "X = pad_sequences(X) \n",
    "print('X.shape = ', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500, 1934)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle = True, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model with embedding layer\n",
    "#flatten to fix the shape error\n",
    "#output is sigmoid with 1 neuron since it is binary classification\n",
    "\n",
    "#embedding help the accuracy reach about 87%, with a dense layer the model stayed at 50-51% the whole time\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=1934))\n",
    "model.add(Flatten())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_24 (Embedding)    (None, 1934, 50)          5069450   \n",
      "                                                                 \n",
      " flatten_22 (Flatten)        (None, 96700)             0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 1)                 96701     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,166,151\n",
      "Trainable params: 5,166,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reduce = ReduceLROnPlateau(monitor='val_loss', mode='min',factor=0.2,patience=3, min_lr=0.001)\n",
    "\n",
    "# my_callbacks = [reduce]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 0.3827 - acc: 0.8227 - val_loss: 0.2895 - val_acc: 0.8774\n",
      "Epoch 2/4\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 0.2451 - acc: 0.9005 - val_loss: 0.2853 - val_acc: 0.8797\n",
      "Epoch 3/4\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 0.1692 - acc: 0.9401 - val_loss: 0.3101 - val_acc: 0.8722\n",
      "Epoch 4/4\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 0.1047 - acc: 0.9706 - val_loss: 0.3412 - val_acc: 0.8689\n"
     ]
    }
   ],
   "source": [
    "#model overfits after a couple epochs because training accuracy gets to 1.0 while val_acc still has ways to go.\n",
    "#A way to fix overfitting would be to reduce learning rate but in about 2 epochs it was capping out so it went up too fast to stabalize\n",
    "with tf.device('/GPU:0'):\n",
    "    history=model.fit(X_train,y_train, epochs=4, verbose=True, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "#import 20 newgroup into a dataframe with the correct target column\n",
    "df = pd.DataFrame([twenty_train.data, twenty_train.target.tolist()]).T\n",
    "df.columns = ['text', 'target']\n",
    "\n",
    "df2 = pd.DataFrame([twenty_test.data, twenty_test.target.tolist()]).T\n",
    "df2.columns = ['text', 'target']\n",
    "\n",
    "concat = [df,df2]\n",
    "\n",
    "df3 = pd.concat(concat,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18841</th>\n",
       "      <td>From: richmond@spiff.Princeton.EDU (Stupendous...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18842</th>\n",
       "      <td>From: smytonj@murr11.alleg.edu (Jim Smyton)\\nS...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18843</th>\n",
       "      <td>From: hhenderson@vax.clarku.edu\\nSubject: RE: ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18844</th>\n",
       "      <td>From: b859zam@utarlg.uta.edu \\nSubject: INTEL ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18845</th>\n",
       "      <td>From: adamsj@gtewd.mtv.gtegsc.com\\nSubject: Re...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18846 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text target\n",
       "0      From: lerxst@wam.umd.edu (where's my thing)\\nS...      7\n",
       "1      From: guykuo@carson.u.washington.edu (Guy Kuo)...      4\n",
       "2      From: twillis@ec.ecn.purdue.edu (Thomas E Will...      4\n",
       "3      From: jgreen@amber (Joe Green)\\nSubject: Re: W...      1\n",
       "4      From: jcm@head-cfa.harvard.edu (Jonathan McDow...     14\n",
       "...                                                  ...    ...\n",
       "18841  From: richmond@spiff.Princeton.EDU (Stupendous...     14\n",
       "18842  From: smytonj@murr11.alleg.edu (Jim Smyton)\\nS...      4\n",
       "18843  From: hhenderson@vax.clarku.edu\\nSubject: RE: ...      9\n",
       "18844  From: b859zam@utarlg.uta.edu \\nSubject: INTEL ...      6\n",
       "18845  From: adamsj@gtewd.mtv.gtegsc.com\\nSubject: Re...     15\n",
       "\n",
       "[18846 rows x 2 columns]"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the texts into a list and set y to the targers\n",
    "sentences = df3['text'].values\n",
    "y = df3['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out the capitals, periods, etc. \n",
    "for i, s in enumerate(sentences):\n",
    "    soup = BeautifulSoup(sentences[i], \"html.parser\")\n",
    "    sentences[i] = soup.get_text()\n",
    "    sentences[i]= re.sub('\\[[^]]*\\]', ' ', sentences[i])\n",
    "    sentences[i] = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    sentences[i] = re.sub(' +', ' ', sentences[i])\n",
    "    sentences[i] = sentences[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the texts\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 2890, 1103, 17, 144, 14, 41, 231, 31, 39, 262, 8, 18, 85, 74, 80, 2890, 1103, 17, 34, 69, 3, 2761, 509, 1595, 32, 6, 27, 1120, 28, 149, 64, 43, 105, 60, 16, 18, 262, 6, 641, 1, 79, 236, 10, 27, 4, 1166, 2226, 262, 1096, 2, 20, 13, 1, 1273, 14, 798, 14, 10, 27, 307, 4, 1, 3650, 73, 165, 447, 7, 1346, 1, 722, 27, 1775, 13, 1, 737, 3, 1, 609, 18, 8, 45, 6, 77, 28, 149, 35, 4, 742, 280, 965, 3031, 193, 3, 2775, 144, 18, 262, 8, 216, 523, 26, 728, 351, 12, 22, 16, 18, 333, 262, 167, 92, 174, 199, 1363, 1282, 2, 12, 37, 56]\n"
     ]
    }
   ],
   "source": [
    "#set the tokens to the X\n",
    "X = tokenizer.texts_to_sequences(sentences)\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape =  (18846, 14111)\n"
     ]
    }
   ],
   "source": [
    "#make them all the same length\n",
    "X = pad_sequences(X) \n",
    "print('X.shape = ', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes sure the inputs are floats\n",
    "X = np.asarray(X).astype('float32')\n",
    "y = np.asarray(y).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14134, 14111)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle = True, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same model but with softmax output for each target, and sparse categorical loss\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=14111))\n",
    "model.add(Flatten())\n",
    "model.add(layers.Dense(20, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_25 (Embedding)    (None, 14111, 50)         5248150   \n",
      "                                                                 \n",
      " flatten_23 (Flatten)        (None, 705550)            0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 20)                14111020  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,359,170\n",
      "Trainable params: 19,359,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "442/442 [==============================] - 14s 31ms/step - loss: 3.1640 - acc: 0.3044 - val_loss: 1.4736 - val_acc: 0.5635\n",
      "Epoch 2/15\n",
      "442/442 [==============================] - 13s 30ms/step - loss: 0.7743 - acc: 0.8426 - val_loss: 0.9297 - val_acc: 0.7330\n",
      "Epoch 3/15\n",
      "442/442 [==============================] - 13s 30ms/step - loss: 0.2972 - acc: 0.9618 - val_loss: 0.8269 - val_acc: 0.7487\n",
      "Epoch 4/15\n",
      "442/442 [==============================] - 14s 32ms/step - loss: 0.1313 - acc: 0.9905 - val_loss: 0.8046 - val_acc: 0.7568\n",
      "Epoch 5/15\n",
      "442/442 [==============================] - 14s 31ms/step - loss: 0.0783 - acc: 0.9972 - val_loss: 0.8015 - val_acc: 0.7564\n",
      "Epoch 6/15\n",
      "442/442 [==============================] - 13s 30ms/step - loss: 0.0528 - acc: 0.9991 - val_loss: 0.7990 - val_acc: 0.7596\n",
      "Epoch 7/15\n",
      "442/442 [==============================] - 13s 31ms/step - loss: 0.0403 - acc: 0.9992 - val_loss: 0.7940 - val_acc: 0.7621\n",
      "Epoch 8/15\n",
      "442/442 [==============================] - 13s 29ms/step - loss: 0.0290 - acc: 0.9994 - val_loss: 0.8044 - val_acc: 0.7610\n",
      "Epoch 9/15\n",
      "442/442 [==============================] - 12s 28ms/step - loss: 0.0176 - acc: 0.9996 - val_loss: 0.8088 - val_acc: 0.7638\n",
      "Epoch 10/15\n",
      "442/442 [==============================] - 13s 29ms/step - loss: 0.0208 - acc: 0.9995 - val_loss: 0.8153 - val_acc: 0.7644\n",
      "Epoch 11/15\n",
      "442/442 [==============================] - 13s 30ms/step - loss: 0.0093 - acc: 0.9996 - val_loss: 0.8205 - val_acc: 0.7623\n",
      "Epoch 12/15\n",
      "442/442 [==============================] - 13s 29ms/step - loss: 0.0135 - acc: 0.9997 - val_loss: 0.8267 - val_acc: 0.7678\n",
      "Epoch 13/15\n",
      "442/442 [==============================] - 13s 30ms/step - loss: 0.0090 - acc: 0.9997 - val_loss: 0.8318 - val_acc: 0.7680\n",
      "Epoch 14/15\n",
      "442/442 [==============================] - 13s 30ms/step - loss: 0.0051 - acc: 0.9998 - val_loss: 0.8425 - val_acc: 0.7710\n",
      "Epoch 15/15\n",
      "442/442 [==============================] - 14s 31ms/step - loss: 0.0017 - acc: 0.9999 - val_loss: 0.8566 - val_acc: 0.7693\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    history=model.fit(X_train,y_train, epochs=15, verbose=True, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max:  7\n",
      "actual:  7.0\n",
      "pred:  [5.8729828e-08 3.5482334e-04 1.0748244e-05 1.5417066e-04 3.9424252e-05\n",
      " 3.5082982e-07 2.1597023e-05 9.9813831e-01 4.2570074e-04 1.7996042e-07\n",
      " 4.3919854e-09 5.6350050e-08 2.9821193e-04 9.6691590e-05 4.5782377e-04\n",
      " 1.7474353e-08 1.1354124e-06 1.3581185e-07 4.5496367e-07 2.1249367e-08]\n"
     ]
    }
   ],
   "source": [
    "#running the predction function\n",
    "y_pred = model.predict(X_test[0:1])\n",
    "\n",
    "#printing the prediction at the index and using argmax to print out the highest predicted value of the output to get result\n",
    "print(\"max: \",np.argmax(y_pred[0]))\n",
    "print(\"actual: \",y_test[0])\n",
    "print(\"pred: \",y_pred[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "435d0e8858e95bf99d49c65e9620e6ce1bbece1b398b9a76f5562c3dd3ad096e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
